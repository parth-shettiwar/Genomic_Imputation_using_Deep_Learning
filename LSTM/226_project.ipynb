{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "226-project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giqpHlHnkEqZ",
        "outputId": "cd20e54d-c06e-401c-9335-51bc8fdf3a15"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive',force_remount=True)\n",
        "import os \n",
        "path = '/content/drive/My Drive/project'\n",
        "os.chdir(path)\n",
        "\n",
        "import numpy as np \n",
        "from numpy.core.fromnumeric import shape\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device: \", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqYDQgzqZT8d"
      },
      "source": [
        "<h1> data stuff </h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBQZJz9AS3Em",
        "outputId": "8da12186-3cc6-4d9a-cf0e-c51267f4b825"
      },
      "source": [
        "!python3 data.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intel MKL FATAL ERROR: Cannot load /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t27R_dyZbPu"
      },
      "source": [
        "class data_load(Dataset):\n",
        "    def __init__(self,train_seq,target_seq):\n",
        "        self.train_seq = train_seq\n",
        "        self.target_seq = target_seq\n",
        "    def __len__(self):\n",
        "        return len(self.train_seq)\n",
        "    def __getitem__(self,idx):\n",
        "        dat = self.train_seq[idx]\n",
        "        gt = self.target_seq[idx]\n",
        "        return dat,gt    \n",
        "\n",
        "\n",
        "def get_features(features, gene):\n",
        "    return (features + gene) % 3\n",
        "\n",
        "def get_data(ref_file, data_file, Nref, Ntrain, Ntest, M, num_chunks):\n",
        "\n",
        "    ref_panel = torch.load(ref_file)\n",
        "    phase_data = torch.load(data_file)\n",
        "    print(ref_panel.shape, phase_data.shape)\n",
        "    my_phase_data = None\n",
        "    for chunk in range(num_chunks):\n",
        "\n",
        "        if my_phase_data is None:\n",
        "            my_phase_data = phase_data[:, 0:500]\n",
        "        else:\n",
        "            my_phase_data = torch.cat((my_phase_data, phase_data[:, chunk * 500 : (chunk + 1)*500]))\n",
        "    print(my_phase_data.shape,\"my phase\")\n",
        "    target_seq = my_phase_data[:, 1:]\n",
        "    my_phase_data = my_phase_data[:,:-1]\n",
        "    print(phase_data.shape,\"phase_data\")\n",
        "    phase_seq = torch.zeros((num_chunks * Ntrain, M - 1, Nref))\n",
        "\n",
        "    for i in range(phase_seq.shape[0]):\n",
        "        ind = i // Ntrain\n",
        "        for j in range(M - 1):\n",
        "            phase_seq[i, j, :] = get_features(ref_panel[:, M*ind + j], phase_data[i % Ntrain, M*ind + j])-1\n",
        "\n",
        "    print(phase_seq.shape, target_seq.shape)\n",
        "\n",
        "    return phase_seq, target_seq\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huuUA-psoBWS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAtQRJsTZpl7"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "Nref = 200\n",
        "Ntrain = 200\n",
        "Ntest = 100\n",
        "M = 500\n",
        "F = 100\n",
        "num_chunks = 1\n",
        "# max_len = 100\n",
        "ref_panel_file = \"data/ref_panel_10000.t\"\n",
        "train_data_file = \"data/train_data_10000.t\"\n",
        "test_data_file = \"data/test_data_10000.t\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiFXZRb6WRvm",
        "outputId": "7b3de898-f754-4771-cf78-f953d2ec70e2"
      },
      "source": [
        "train_seq, target_seq = get_data(ref_panel_file, train_data_file, Nref, Ntrain, Ntest, M, num_chunks)\n",
        "#val_seq, target_seq_val = get_data(ref_panel_file, test_data_file, Nref, Ntrain, Ntest, M, num_chunks)\n",
        "train_data = data_load(train_seq,target_seq)\n",
        "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([200, 10000]) torch.Size([200, 10000])\n",
            "torch.Size([200, 500]) my phase\n",
            "torch.Size([200, 10000]) phase_data\n",
            "torch.Size([200, 499, 200]) torch.Size([200, 499])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvneICyZn6B_"
      },
      "source": [
        "<h1> model stuff </h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcIVrQKPcxd9"
      },
      "source": [
        "def accuracy(lst1, lst2):\n",
        "    n = len(lst1)\n",
        "    return np.sum(lst1 == lst2)/n\n",
        "\n",
        "def accuracy_torch(lst1, lst2):\n",
        "    n = len(lst1)\n",
        "    return torch.sum(lst1 == lst2)/n\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers, embed_size=64):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embed_layer = nn.Linear(input_size, embed_size)\n",
        "        # self.rnn = nn.RNN(embed_size, hidden_dim, n_layers, batch_first=True)   \n",
        "        self.rnn = nn.LSTM(embed_size, hidden_dim, n_layers, batch_first=True,bidirectional=False)\n",
        "        self.fc = nn.Linear(1*hidden_dim, output_size)\n",
        "        \n",
        "        self.sigmoid = nn.LogSoftmax()\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # fvefrdsc\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "        # Initializing hidden state for first input using method defined below\n",
        "        # hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        x = self.embed_layer(x)\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        out, hidden = self.rnn(x)\n",
        "        # print(out.shape,\"mmm\")\n",
        "        # out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, 1*self.hidden_dim)\n",
        "        # print(out.shape,\"dde\")\n",
        "        out = self.fc(out)\n",
        "        out = self.relu(out)\n",
        "        \n",
        "        return out, hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJwD5sV0gXO6"
      },
      "source": [
        "def sample_val(model, out_len, start, chunk):\n",
        "\n",
        "    m = len(start)\n",
        "    train_seq = torch.zeros((1, m, Nref))\n",
        "\n",
        "    for i in range(train_seq.shape[0]):\n",
        "        for j in range(m):\n",
        "            train_seq[i, j, :] = get_features(ref_panel[:, M * chunk + j], start[j]) - 1\n",
        "\n",
        "    model.eval()\n",
        "    ind = len(start)\n",
        "    dosages = []\n",
        "    for _ in range(out_len - len(start)):\n",
        "        train_seq = train_seq.to(\"cuda\")\n",
        "        out, hidden = model(train_seq)\n",
        "        prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "        dosages.append(prob[0] * 0.0 + prob[1] * 1.0 + prob[2] * 2.0)\n",
        "        # print(prob)\n",
        "        gene = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "        start.append(gene)\n",
        "        feat = get_features(ref_panel[:, M * chunk + ind], gene)\n",
        "        feat = feat.reshape(1, 1, len(feat)).to(\"cuda\") - 1\n",
        "        ind += 1\n",
        "        train_seq = torch.cat((train_seq, feat), dim=1)\n",
        "    \n",
        "    dosages = [i.item() for i in dosages]\n",
        "    print(dosages)\n",
        "    return torch.tensor(start), dosages\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2BZHXh-TqeS",
        "outputId": "226b1643-dd51-44cb-ba89-eef9ef393738"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzIJE52JgeQp"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def validate(model,test_data, n_val, F, num_chunks=num_chunks):  \n",
        "    total_acc = 0.0\n",
        "    total_r2 = 0.0\n",
        "    c = 0\n",
        "    i = 0\n",
        "\n",
        "    for data in test_data:\n",
        "        i +=1\n",
        "        print(\"dataa\", data.shape)\n",
        "        dat = data.tolist()\n",
        "        local_acc = 0.0\n",
        "        local_r2 = 0.0\n",
        "        for chunk in (range(num_chunks)):\n",
        "          inp_data = dat[M * chunk : M * chunk + F]\n",
        "          generated, dosages = sample_val(model, M, inp_data, chunk)\n",
        "          # print(generated)\n",
        "          acc = accuracy(np.array(generated[F:]), np.array(dat[M * chunk + F: M * (chunk + 1)]))\n",
        "          # r2 = r2_score(np.array(dat[M * chunk + F: M * (chunk + 1)]), np.array(dosages))\n",
        "          r2 = r2_score(np.random.randint(0, 3, (M-F)), np.array(dosages))\n",
        "          # print(generated, dat)\n",
        "          local_acc += acc\n",
        "          local_r2 += r2\n",
        "        local_acc /= num_chunks\n",
        "        local_r2 /= num_chunks\n",
        "        print(c, \"Accuracy: \", local_acc)\n",
        "        print(c, \"Avg R2:\", local_r2)\n",
        "        c += 1\n",
        "        total_acc += local_acc\n",
        "        total_r2 += local_r2\n",
        "        if i == n_val or i == len(test_data):\n",
        "          break\n",
        "    print(\"Average Acc: \", total_acc/i)\n",
        "    print(\"Average R2:\", total_r2/i)\n",
        "    return total_acc/i\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4DhudeWVV_a"
      },
      "source": [
        "# from model import Model\n",
        "torch.manual_seed(0)\n",
        "model = Model(input_size=Nref, output_size=3, hidden_dim=32, n_layers=1, embed_size=32)\n",
        "model = model.to(device)\n",
        "\n",
        "n_epochs = 200\n",
        "lr=1e-3\n",
        "\n",
        "n_val = 1\n",
        "test_data = torch.load(\"data/test_data_1kg.t\")\n",
        "ref_panel = torch.load(ref_panel_file)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr,betas=(0.5, 0.999))\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEOYygPtVX8P"
      },
      "source": [
        "print(device)\n",
        "# optimizer.to(device)\n",
        "# Training Run\n",
        "\n",
        "val_acc = -1\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, lo = 0.0, 0\n",
        "    for mat,gt in tqdm(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        mat = mat.to(device)\n",
        "        gt = gt.to(device)\n",
        "        output, hidden = model(mat)\n",
        "        loss = criterion(output, gt.contiguous().view(-1).long())\n",
        "        prob = nn.functional.softmax(output, dim=0).data\n",
        "        gene = torch.max(prob, dim=1)[1]\n",
        "        acc = accuracy(np.array(gene.cpu().detach()), np.array(gt.contiguous().view(-1).long().cpu().detach()))\n",
        "        # acc = accuracy_torch(gene, gt.contiguous().view(-1)).item()\n",
        "        lo += acc\n",
        "\n",
        "        loss.backward() # Does backpropagation and calculates gradients\n",
        "        optimizer.step() # Updates the weights accordingly\n",
        "        #scheduler.step()\n",
        "\n",
        "    if epoch%10 == 1:\n",
        "        cur_val_acc = validate(model,test_data,n_val)\n",
        "        print(\"val acc: \", cur_val_acc)\n",
        "        if cur_val_acc > val_acc:\n",
        "            print('saving in epoch', epoch)\n",
        "            val_acc = cur_val_acc\n",
        "            print(f'new val acc: {val_acc}')\n",
        "            torch.save(model, \"model_best_lstm_art.pth\")\n",
        "\n",
        "    if epoch%1 == 0:\n",
        "        print(\"train acc\",lo/len(train_dataloader))\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"train Loss: {:.4f}\".format(loss.item()))\n",
        "        print(\"val Acc: \", val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "OMttDraS1HAa",
        "outputId": "25a4b4fa-6933-4f3b-fc33-2324ef90b44a"
      },
      "source": [
        "test_data = torch.load(\"data/test_data_1kg.t\")\n",
        "# model_best = torch.load(\"model_best_lstm_94_1kg.pth\")\n",
        "model_best = torch.load(\"model_62.pth\")\n",
        "for F in [100]:\n",
        "  final_val_acc = validate(model_best,test_data,len(test_data), F=F, num_chunks=1)\n",
        "  print(final_val_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataa torch.Size([10000])\n",
            "[0.8504234552383423, 1.2164604663848877, 1.1229243278503418, 1.766209363937378, 0.8622874617576599, 0.874392032623291, 1.5882911682128906, 0.6414315104484558, 0.0830097571015358, 0.25102925300598145, 1.9052636623382568, 1.3642237186431885, 0.7462375164031982, 0.14893363416194916, 1.5016674995422363, 0.42242151498794556, 1.9465006589889526, 0.6122291684150696, 0.7077105045318604, 1.68416428565979, 0.12826655805110931, 1.8862810134887695, 0.5964319705963135, 1.3950039148330688, 1.982553482055664, 1.9250726699829102, 0.7456279397010803, 0.26129046082496643, 1.9481264352798462, 0.82900071144104, 1.6233117580413818, 1.060192346572876, 1.7314257621765137, 0.7958199977874756, 1.5867650508880615, 0.9363148212432861, 1.8453024625778198, 1.3563956022262573, 1.635040521621704, 0.3212730288505554, 0.3933440148830414, 0.9829307794570923, 1.8138542175292969, 1.8182430267333984, 0.21424923837184906, 1.9242627620697021, 0.9553394317626953, 1.7158279418945312, 0.654747724533081, 1.466010570526123, 0.7747886180877686, 0.1191343292593956, 1.8948134183883667, 0.24210727214813232, 0.7424220442771912, 0.10354942828416824, 1.5418720245361328, 0.9137130975723267, 1.1157948970794678, 1.9564430713653564, 1.604526400566101, 1.0596647262573242, 1.0590851306915283, 0.17468954622745514, 1.1297004222869873, 0.36781883239746094, 0.1659727245569229, 1.4639443159103394, 1.1222938299179077, 1.1537200212478638, 1.1348412036895752, 0.804531455039978, 0.40116071701049805, 0.11069536209106445, 0.8781765103340149, 1.6484999656677246, 0.35224467515945435, 0.9041489362716675, 0.546176552772522, 0.46328091621398926, 1.0812073945999146, 1.3558167219161987, 1.5850441455841064, 0.3551085293292999, 0.9886376857757568, 1.250279426574707, 0.5277792811393738, 1.2112360000610352, 1.1765620708465576, 1.3029531240463257, 1.8868796825408936, 1.2632616758346558, 0.09883833676576614, 0.09076361358165741, 0.746274471282959, 0.5953083634376526, 1.8010683059692383, 1.1773896217346191, 1.1471586227416992, 1.2540440559387207, 0.2831507921218872, 0.7673813104629517, 1.5036219358444214, 0.8593188524246216, 1.1108801364898682, 0.7550427913665771, 1.9190620183944702, 0.6114852428436279, 1.9538670778274536, 0.3258228600025177, 0.16272999346256256, 1.5098406076431274, 1.875441312789917, 0.608849287033081, 1.585323691368103, 0.10080421715974808, 0.2805624008178711, 1.4750529527664185, 0.7741959095001221, 0.2587988078594208, 0.2713984251022339, 1.741860270500183, 0.8042584657669067, 1.6333534717559814, 0.6059356927871704, 0.0982321947813034, 1.7458807229995728, 0.8609026670455933, 1.0815560817718506, 1.573052167892456, 1.7881150245666504, 1.6931965351104736, 0.09422272443771362, 1.6548621654510498, 1.7468721866607666, 0.37840771675109863, 0.08327251672744751, 0.9448880553245544, 0.21407808363437653, 1.5031764507293701, 0.44016674160957336, 0.05287975072860718, 1.0451247692108154, 0.06996755301952362, 1.7087583541870117, 1.6151729822158813, 1.4307854175567627, 1.1074118614196777, 1.8275563716888428, 1.2653554677963257, 0.9325695037841797, 0.7901332378387451, 0.23736447095870972, 1.9178880453109741, 0.35502690076828003, 1.339095950126648, 1.9699143171310425, 0.18568390607833862, 0.244188591837883, 1.3778560161590576, 1.669215202331543, 0.05125468224287033, 0.6949565410614014, 1.381073236465454, 0.7024333477020264, 1.3256055116653442, 1.2282980680465698, 0.21504418551921844, 0.9742445349693298, 1.6998341083526611, 1.5270167589187622, 0.8016185760498047, 0.7178796529769897, 0.6594429612159729, 0.08111988008022308, 0.4470289945602417, 0.15733300149440765, 0.34886088967323303, 0.4743386209011078, 1.7349255084991455, 1.0259859561920166, 1.2933063507080078, 1.2473100423812866, 0.3478473722934723, 1.460350513458252, 0.3598458468914032, 0.9616078734397888, 1.5230194330215454, 0.032315149903297424, 1.5584521293640137, 0.04062015935778618, 1.1152405738830566, 0.6277321577072144, 1.0115928649902344, 1.6213194131851196, 1.5579533576965332, 0.30894869565963745, 1.4359986782073975, 0.4562261700630188, 0.9683132767677307, 1.6134753227233887, 0.9708189964294434, 1.087467074394226, 0.6331477165222168, 1.3595726490020752, 1.058562994003296, 0.569861650466919, 0.3259725868701935, 1.3638958930969238, 1.8894175291061401, 0.5783735513687134, 1.866523265838623, 0.6551662683486938, 0.2500542402267456, 0.07896407693624496, 0.7595462799072266, 0.9021350145339966, 1.2636083364486694, 1.4552645683288574, 0.11167863011360168, 0.6210852861404419, 1.2367157936096191, 0.2312253713607788, 1.949583888053894, 0.6368702054023743, 1.8737534284591675, 1.583132266998291, 0.622467041015625, 0.752066433429718, 0.04636433348059654, 0.6526839733123779, 0.8889609575271606, 0.1063506007194519, 1.5443254709243774, 1.1006896495819092, 0.7459495067596436, 0.9429059028625488, 0.10927598923444748, 0.8850541114807129, 0.5354952216148376, 0.639714777469635, 0.3931232988834381, 1.7220571041107178, 0.456825852394104, 1.8112006187438965, 0.19134199619293213, 1.2016842365264893, 0.025839224457740784, 1.4445409774780273, 1.3975646495819092, 0.7417455911636353, 1.5555732250213623, 1.9751129150390625, 1.613784909248352, 0.6222553849220276, 0.17938484251499176, 1.7455735206604004, 0.389529824256897, 1.2435345649719238, 1.1059777736663818, 1.0839087963104248, 1.0441720485687256, 1.5595016479492188, 1.417967677116394, 0.35199475288391113, 0.4487910866737366, 0.5071582794189453, 0.47406530380249023, 1.3917280435562134, 1.7013964653015137, 1.3999829292297363, 0.7986407279968262, 1.1429269313812256, 1.876501202583313, 1.6473373174667358, 1.013404130935669, 0.18161651492118835, 0.4771818518638611, 1.8881938457489014, 1.4939279556274414, 0.912609338760376, 0.7349482774734497, 0.21695415675640106, 0.09235358983278275, 0.8508121371269226, 1.3346792459487915, 0.3203356862068176, 0.6059083938598633, 0.03916699439287186, 1.6725884675979614, 1.4593199491500854, 1.0309460163116455, 0.6583412885665894, 1.6937353610992432, 0.5364249348640442, 0.1568189263343811, 0.5660985112190247, 0.7853900790214539, 1.5594451427459717, 0.5589738488197327, 0.15896840393543243, 0.7247354388237, 1.810834288597107, 0.14945296943187714, 0.13781780004501343, 1.07326340675354, 0.3668963313102722, 0.289782851934433, 1.7163935899734497, 0.21147319674491882, 1.8700168132781982, 1.9292656183242798, 0.37895631790161133, 0.807747483253479, 1.9074132442474365, 1.2556706666946411, 0.6309916377067566, 1.7245409488677979, 0.15512816607952118, 1.2992305755615234, 0.4622032642364502, 0.761527419090271, 0.5223717093467712, 1.0427613258361816, 1.3892982006072998, 1.168886423110962, 0.247366800904274, 0.5821149945259094, 1.4970000982284546, 0.18379747867584229, 0.2859250009059906, 0.3046190142631531, 0.5951294302940369, 1.1754910945892334, 0.6200870275497437, 1.1789841651916504, 0.8682563900947571, 0.16649635136127472, 1.5374598503112793, 1.272608757019043, 1.7372676134109497, 1.2919179201126099, 1.6755489110946655, 1.5368058681488037, 0.8147503137588501, 0.39209431409835815, 0.14643436670303345, 1.3374015092849731, 0.4146162271499634, 0.7963175773620605, 0.6032490730285645, 1.181990385055542, 0.07489119470119476, 1.6496752500534058, 1.1694667339324951, 0.7131495475769043, 0.35209622979164124, 1.8714768886566162, 1.0802743434906006, 1.880111575126648, 0.444853812456131, 0.16080276668071747, 0.11318971961736679, 0.6552122235298157, 0.9645576477050781, 1.5736064910888672, 1.3064974546432495, 0.0729420930147171, 1.7268489599227905, 1.3974889516830444, 1.7502529621124268, 0.5958396792411804, 0.4568121135234833, 1.6354951858520508, 1.012461543083191, 0.7435727119445801, 1.284142017364502, 0.6760649681091309, 0.7418264150619507, 0.13181518018245697, 1.7085604667663574, 1.8316683769226074, 1.1100914478302002, 1.4697061777114868, 0.05595321208238602, 0.4894207715988159, 1.1429612636566162, 0.04681345447897911, 1.1806676387786865, 1.2206223011016846, 1.0347508192062378, 1.1789546012878418, 1.6138887405395508, 1.8606431484222412, 0.2607283890247345, 1.2290141582489014, 1.5306611061096191, 0.7890139818191528, 0.8493375778198242, 0.8613803386688232]\n",
            "0 Accuracy:  0.345\n",
            "0 Avg R2: -0.5790120192149171\n",
            "dataa torch.Size([10000])\n",
            "[0.8504235148429871, 1.2164603471755981, 1.1229243278503418, 1.766209363937378, 0.8622874617576599, 0.874392032623291, 1.5882911682128906, 0.6414315104484558, 0.0830097571015358, 0.25102925300598145, 1.9052636623382568, 1.3642237186431885, 0.7462375164031982, 0.14893363416194916, 1.5016674995422363, 0.42242151498794556, 1.9465006589889526, 0.6122291684150696, 0.7077105045318604, 1.68416428565979, 0.12826655805110931, 1.8862810134887695, 0.5964319705963135, 1.3950039148330688, 1.982553482055664, 1.9250726699829102, 0.7456279397010803, 0.26129046082496643, 1.9481264352798462, 0.82900071144104, 1.6233117580413818, 1.060192346572876, 1.7314257621765137, 0.7958199977874756, 1.5867650508880615, 0.9363148212432861, 1.8453024625778198, 1.3563956022262573, 1.635040521621704, 0.3212730288505554, 0.3933440148830414, 0.9829307794570923, 1.8138542175292969, 1.8182430267333984, 0.21424923837184906, 1.9242627620697021, 0.9553394317626953, 1.7158279418945312, 0.654747724533081, 1.466010570526123, 0.7747886180877686, 0.1191343292593956, 1.8948134183883667, 0.24210727214813232, 0.7424220442771912, 0.10354942828416824, 1.5418720245361328, 0.9137130975723267, 1.1157948970794678, 1.9564430713653564, 1.604526400566101, 1.0596647262573242, 1.0590851306915283, 0.17468954622745514, 1.1297004222869873, 0.36781883239746094, 0.1659727245569229, 1.4639443159103394, 1.1222938299179077, 1.1537200212478638, 1.1348412036895752, 0.804531455039978, 0.40116071701049805, 0.11069536209106445, 0.8781765103340149, 1.6484999656677246, 0.35224467515945435, 0.9041489362716675, 0.546176552772522, 0.46328091621398926, 1.0812073945999146, 1.3558167219161987, 1.5850441455841064, 0.3551085293292999, 0.9886376857757568, 1.250279426574707, 0.5277792811393738, 1.2112360000610352, 1.1765620708465576, 1.3029531240463257, 1.8868796825408936, 1.2632616758346558, 0.09883833676576614, 0.09076361358165741, 0.746274471282959, 0.5953083634376526, 1.8010683059692383, 1.1773896217346191, 1.1471586227416992, 1.2540440559387207, 0.2831507921218872, 0.7673813104629517, 1.5036219358444214, 0.8593188524246216, 1.1108801364898682, 0.7550427913665771, 1.9190620183944702, 0.6114852428436279, 1.9538670778274536, 0.3258228600025177, 0.16272999346256256, 1.5098406076431274, 1.875441312789917, 0.608849287033081, 1.585323691368103, 0.10080421715974808, 0.2805624008178711, 1.4750529527664185, 0.7741959095001221, 0.2587988078594208, 0.2713984251022339, 1.741860270500183, 0.8042584657669067, 1.6333534717559814, 0.6059356927871704, 0.0982321947813034, 1.7458807229995728, 0.8609026670455933, 1.0815560817718506, 1.573052167892456, 1.7881150245666504, 1.6931965351104736, 0.09422272443771362, 1.6548621654510498, 1.7468721866607666, 0.37840771675109863, 0.08327251672744751, 0.9448880553245544, 0.21407808363437653, 1.5031764507293701, 0.44016674160957336, 0.05287975072860718, 1.0451247692108154, 0.06996755301952362, 1.7087583541870117, 1.6151729822158813, 1.4307854175567627, 1.1074118614196777, 1.8275563716888428, 1.2653554677963257, 0.9325695037841797, 0.7901332378387451, 0.23736447095870972, 1.9178880453109741, 0.35502690076828003, 1.339095950126648, 1.9699143171310425, 0.18568390607833862, 0.244188591837883, 1.3778560161590576, 1.669215202331543, 0.05125468224287033, 0.6949565410614014, 1.381073236465454, 0.7024333477020264, 1.3256055116653442, 1.2282980680465698, 0.21504418551921844, 0.9742445349693298, 1.6998341083526611, 1.5270167589187622, 0.8016185760498047, 0.7178796529769897, 0.6594429612159729, 0.08111988008022308, 0.4470289945602417, 0.15733300149440765, 0.34886088967323303, 0.4743386209011078, 1.7349255084991455, 1.0259859561920166, 1.2933063507080078, 1.2473100423812866, 0.3478473722934723, 1.460350513458252, 0.3598458468914032, 0.9616078734397888, 1.5230194330215454, 0.032315149903297424, 1.5584521293640137, 0.04062015935778618, 1.1152405738830566, 0.6277321577072144, 1.0115928649902344, 1.6213194131851196, 1.5579533576965332, 0.30894869565963745, 1.4359986782073975, 0.4562261700630188, 0.9683132767677307, 1.6134753227233887, 0.9708189964294434, 1.087467074394226, 0.6331477165222168, 1.3595726490020752, 1.058562994003296, 0.569861650466919, 0.3259725868701935, 1.3638958930969238, 1.8894175291061401, 0.5783735513687134, 1.866523265838623, 0.6551662683486938, 0.2500542402267456, 0.07896407693624496, 0.7595462799072266, 0.9021350145339966, 1.2636083364486694, 1.4552645683288574, 0.11167863011360168, 0.6210852861404419, 1.2367157936096191, 0.2312253713607788, 1.949583888053894, 0.6368702054023743, 1.8737534284591675, 1.583132266998291, 0.622467041015625, 0.752066433429718, 0.04636433348059654, 0.6526839733123779, 0.8889609575271606, 0.1063506007194519, 1.5443254709243774, 1.1006896495819092, 0.7459495067596436, 0.9429059028625488, 0.10927598923444748, 0.8850541114807129, 0.5354952216148376, 0.639714777469635, 0.3931232988834381, 1.7220571041107178, 0.456825852394104, 1.8112006187438965, 0.19134199619293213, 1.2016842365264893, 0.025839224457740784, 1.4445409774780273, 1.3975646495819092, 0.7417455911636353, 1.5555732250213623, 1.9751129150390625, 1.613784909248352, 0.6222553849220276, 0.17938484251499176, 1.7455735206604004, 0.389529824256897, 1.2435345649719238, 1.1059777736663818, 1.0839087963104248, 1.0441720485687256, 1.5595016479492188, 1.417967677116394, 0.35199475288391113, 0.4487910866737366, 0.5071582794189453, 0.47406530380249023, 1.3917280435562134, 1.7013964653015137, 1.3999829292297363, 0.7986407279968262, 1.1429269313812256, 1.876501202583313, 1.6473373174667358, 1.013404130935669, 0.18161651492118835, 0.4771818518638611, 1.8881938457489014, 1.4939279556274414, 0.912609338760376, 0.7349482774734497, 0.21695415675640106, 0.09235358983278275, 0.8508121371269226, 1.3346792459487915, 0.3203356862068176, 0.6059083938598633, 0.03916699439287186, 1.6725884675979614, 1.4593199491500854, 1.0309460163116455, 0.6583412885665894, 1.6937353610992432, 0.5364249348640442, 0.1568189263343811, 0.5660985112190247, 0.7853900790214539, 1.5594451427459717, 0.5589738488197327, 0.15896840393543243, 0.7247354388237, 1.810834288597107, 0.14945296943187714, 0.13781780004501343, 1.07326340675354, 0.3668963313102722, 0.289782851934433, 1.7163935899734497, 0.21147319674491882, 1.8700168132781982, 1.9292656183242798, 0.37895631790161133, 0.807747483253479, 1.9074132442474365, 1.2556706666946411, 0.6309916377067566, 1.7245409488677979, 0.15512816607952118, 1.2992305755615234, 0.4622032642364502, 0.761527419090271, 0.5223717093467712, 1.0427613258361816, 1.3892982006072998, 1.168886423110962, 0.247366800904274, 0.5821149945259094, 1.4970000982284546, 0.18379747867584229, 0.2859250009059906, 0.3046190142631531, 0.5951294302940369, 1.1754910945892334, 0.6200870275497437, 1.1789841651916504, 0.8682563900947571, 0.16649635136127472, 1.5374598503112793, 1.272608757019043, 1.7372676134109497, 1.2919179201126099, 1.6755489110946655, 1.5368058681488037, 0.8147503137588501, 0.39209431409835815, 0.14643436670303345, 1.3374015092849731, 0.4146162271499634, 0.7963175773620605, 0.6032490730285645, 1.181990385055542, 0.07489119470119476, 1.6496752500534058, 1.1694667339324951, 0.7131495475769043, 0.35209622979164124, 1.8714768886566162, 1.0802743434906006, 1.880111575126648, 0.444853812456131, 0.16080276668071747, 0.11318971961736679, 0.6552122235298157, 0.9645576477050781, 1.5736064910888672, 1.3064974546432495, 0.0729420930147171, 1.7268489599227905, 1.3974889516830444, 1.7502529621124268, 0.5958396792411804, 0.4568121135234833, 1.6354951858520508, 1.012461543083191, 0.7435727119445801, 1.284142017364502, 0.6760649681091309, 0.7418264150619507, 0.13181518018245697, 1.7085604667663574, 1.8316683769226074, 1.1100914478302002, 1.4697061777114868, 0.05595321208238602, 0.4894207715988159, 1.1429612636566162, 0.04681345447897911, 1.1806676387786865, 1.2206223011016846, 1.0347508192062378, 1.1789546012878418, 1.6138887405395508, 1.8606431484222412, 0.2607283890247345, 1.2290141582489014, 1.5306611061096191, 0.7890139818191528, 0.8493375778198242, 0.8613803386688232]\n",
            "1 Accuracy:  0.3525\n",
            "1 Avg R2: -0.4175242757362394\n",
            "dataa torch.Size([10000])\n",
            "[0.8504234552383423, 1.2164604663848877, 1.1229243278503418, 1.766209363937378, 0.8622874617576599, 0.874392032623291, 1.5882911682128906, 0.6414315104484558, 0.0830097571015358, 0.25102925300598145, 1.9052636623382568, 1.3642237186431885, 0.7462375164031982, 0.14893363416194916, 1.5016674995422363, 0.42242151498794556, 1.9465006589889526, 0.6122291684150696, 0.7077105045318604, 1.68416428565979, 0.12826655805110931, 1.8862810134887695, 0.5964319705963135, 1.3950039148330688, 1.982553482055664, 1.9250726699829102, 0.7456279397010803, 0.26129046082496643, 1.9481264352798462, 0.82900071144104, 1.6233117580413818, 1.060192346572876, 1.7314257621765137, 0.7958199977874756, 1.5867650508880615, 0.9363148212432861, 1.8453024625778198, 1.3563956022262573, 1.635040521621704, 0.3212730288505554, 0.3933440148830414, 0.9829307794570923, 1.8138542175292969, 1.8182430267333984, 0.21424923837184906, 1.9242627620697021, 0.9553394317626953, 1.7158279418945312, 0.654747724533081, 1.466010570526123, 0.7747886180877686, 0.1191343292593956, 1.8948134183883667, 0.24210727214813232, 0.7424220442771912, 0.10354942828416824, 1.5418720245361328, 0.9137130975723267, 1.1157948970794678, 1.9564430713653564, 1.604526400566101, 1.0596647262573242, 1.0590851306915283, 0.17468954622745514, 1.1297004222869873, 0.36781883239746094, 0.1659727245569229, 1.4639443159103394, 1.1222938299179077, 1.1537200212478638, 1.1348412036895752, 0.804531455039978, 0.40116071701049805, 0.11069536209106445, 0.8781765103340149, 1.6484999656677246, 0.35224467515945435, 0.9041489362716675, 0.546176552772522, 0.46328091621398926, 1.0812073945999146, 1.3558167219161987, 1.5850441455841064, 0.3551085293292999, 0.9886376857757568, 1.250279426574707, 0.5277792811393738, 1.2112360000610352, 1.1765620708465576, 1.3029531240463257, 1.8868796825408936, 1.2632616758346558, 0.09883833676576614, 0.09076361358165741, 0.746274471282959, 0.5953083634376526, 1.8010683059692383, 1.1773896217346191, 1.1471586227416992, 1.2540440559387207, 0.2831507921218872, 0.7673813104629517, 1.5036219358444214, 0.8593188524246216, 1.1108801364898682, 0.7550427913665771, 1.9190620183944702, 0.6114852428436279, 1.9538670778274536, 0.3258228600025177, 0.16272999346256256, 1.5098406076431274, 1.875441312789917, 0.608849287033081, 1.585323691368103, 0.10080421715974808, 0.2805624008178711, 1.4750529527664185, 0.7741959095001221, 0.2587988078594208, 0.2713984251022339, 1.741860270500183, 0.8042584657669067, 1.6333534717559814, 0.6059356927871704, 0.0982321947813034, 1.7458807229995728, 0.8609026670455933, 1.0815560817718506, 1.573052167892456, 1.7881150245666504, 1.6931965351104736, 0.09422272443771362, 1.6548621654510498, 1.7468721866607666, 0.37840771675109863, 0.08327251672744751, 0.9448880553245544, 0.21407808363437653, 1.5031764507293701, 0.44016674160957336, 0.05287975072860718, 1.0451247692108154, 0.06996755301952362, 1.7087583541870117, 1.6151729822158813, 1.4307854175567627, 1.1074118614196777, 1.8275563716888428, 1.2653554677963257, 0.9325695037841797, 0.7901332378387451, 0.23736447095870972, 1.9178880453109741, 0.35502690076828003, 1.339095950126648, 1.9699143171310425, 0.18568390607833862, 0.244188591837883, 1.3778560161590576, 1.669215202331543, 0.05125468224287033, 0.6949565410614014, 1.381073236465454, 0.7024333477020264, 1.3256055116653442, 1.2282980680465698, 0.21504418551921844, 0.9742445349693298, 1.6998341083526611, 1.5270167589187622, 0.8016185760498047, 0.7178796529769897, 0.6594429612159729, 0.08111988008022308, 0.4470289945602417, 0.15733300149440765, 0.34886088967323303, 0.4743386209011078, 1.7349255084991455, 1.0259859561920166, 1.2933063507080078, 1.2473100423812866, 0.3478473722934723, 1.460350513458252, 0.3598458468914032, 0.9616078734397888, 1.5230194330215454, 0.032315149903297424, 1.5584521293640137, 0.04062015935778618, 1.1152405738830566, 0.6277321577072144, 1.0115928649902344, 1.6213194131851196, 1.5579533576965332, 0.30894869565963745, 1.4359986782073975, 0.4562261700630188, 0.9683132767677307, 1.6134753227233887, 0.9708189964294434, 1.087467074394226, 0.6331477165222168, 1.3595726490020752, 1.058562994003296, 0.569861650466919, 0.3259725868701935, 1.3638958930969238, 1.8894175291061401, 0.5783735513687134, 1.866523265838623, 0.6551662683486938, 0.2500542402267456, 0.07896407693624496, 0.7595462799072266, 0.9021350145339966, 1.2636083364486694, 1.4552645683288574, 0.11167863011360168, 0.6210852861404419, 1.2367157936096191, 0.2312253713607788, 1.949583888053894, 0.6368702054023743, 1.8737534284591675, 1.583132266998291, 0.622467041015625, 0.752066433429718, 0.04636433348059654, 0.6526839733123779, 0.8889609575271606, 0.1063506007194519, 1.5443254709243774, 1.1006896495819092, 0.7459495067596436, 0.9429059028625488, 0.10927598923444748, 0.8850541114807129, 0.5354952216148376, 0.639714777469635, 0.3931232988834381, 1.7220571041107178, 0.456825852394104, 1.8112006187438965, 0.19134199619293213, 1.2016842365264893, 0.025839224457740784, 1.4445409774780273, 1.3975646495819092, 0.7417455911636353, 1.5555732250213623, 1.9751129150390625, 1.613784909248352, 0.6222553849220276, 0.17938484251499176, 1.7455735206604004, 0.389529824256897, 1.2435345649719238, 1.1059777736663818, 1.0839087963104248, 1.0441720485687256, 1.5595016479492188, 1.417967677116394, 0.35199475288391113, 0.4487910866737366, 0.5071582794189453, 0.47406530380249023, 1.3917280435562134, 1.7013964653015137, 1.3999829292297363, 0.7986407279968262, 1.1429269313812256, 1.876501202583313, 1.6473373174667358, 1.013404130935669, 0.18161651492118835, 0.4771818518638611, 1.8881938457489014, 1.4939279556274414, 0.912609338760376, 0.7349482774734497, 0.21695415675640106, 0.09235358983278275, 0.8508121371269226, 1.3346792459487915, 0.3203356862068176, 0.6059083938598633, 0.03916699439287186, 1.6725884675979614, 1.4593199491500854, 1.0309460163116455, 0.6583412885665894, 1.6937353610992432, 0.5364249348640442, 0.1568189263343811, 0.5660985112190247, 0.7853900790214539, 1.5594451427459717, 0.5589738488197327, 0.15896840393543243, 0.7247354388237, 1.810834288597107, 0.14945296943187714, 0.13781780004501343, 1.07326340675354, 0.3668963313102722, 0.289782851934433, 1.7163935899734497, 0.21147319674491882, 1.8700168132781982, 1.9292656183242798, 0.37895631790161133, 0.807747483253479, 1.9074132442474365, 1.2556706666946411, 0.6309916377067566, 1.7245409488677979, 0.15512816607952118, 1.2992305755615234, 0.4622032642364502, 0.761527419090271, 0.5223717093467712, 1.0427613258361816, 1.3892982006072998, 1.168886423110962, 0.247366800904274, 0.5821149945259094, 1.4970000982284546, 0.18379747867584229, 0.2859250009059906, 0.3046190142631531, 0.5951294302940369, 1.1754910945892334, 0.6200870275497437, 1.1789841651916504, 0.8682563900947571, 0.16649635136127472, 1.5374598503112793, 1.272608757019043, 1.7372676134109497, 1.2919179201126099, 1.6755489110946655, 1.5368058681488037, 0.8147503137588501, 0.39209431409835815, 0.14643436670303345, 1.3374015092849731, 0.4146162271499634, 0.7963175773620605, 0.6032490730285645, 1.181990385055542, 0.07489119470119476, 1.6496752500534058, 1.1694667339324951, 0.7131495475769043, 0.35209622979164124, 1.8714768886566162, 1.0802743434906006, 1.880111575126648, 0.444853812456131, 0.16080276668071747, 0.11318971961736679, 0.6552122235298157, 0.9645576477050781, 1.5736064910888672, 1.3064974546432495, 0.0729420930147171, 1.7268489599227905, 1.3974889516830444, 1.7502529621124268, 0.5958396792411804, 0.4568121135234833, 1.6354951858520508, 1.012461543083191, 0.7435727119445801, 1.284142017364502, 0.6760649681091309, 0.7418264150619507, 0.13181518018245697, 1.7085604667663574, 1.8316683769226074, 1.1100914478302002, 1.4697061777114868, 0.05595321208238602, 0.4894207715988159, 1.1429612636566162, 0.04681345447897911, 1.1806676387786865, 1.2206223011016846, 1.0347508192062378, 1.1789546012878418, 1.6138887405395508, 1.8606431484222412, 0.2607283890247345, 1.2290141582489014, 1.5306611061096191, 0.7890139818191528, 0.8493375778198242, 0.8613803386688232]\n",
            "2 Accuracy:  0.3325\n",
            "2 Avg R2: -0.5608568638172438\n",
            "dataa torch.Size([10000])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-845a85d23ebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_62.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mF\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mfinal_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_val_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-b700b710e99d>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, test_data, n_val, F, num_chunks)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0minp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m           \u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdosages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m           \u001b[0;31m# print(generated)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-74dcb9b094e5>\u001b[0m in \u001b[0;36msample_val\u001b[0;34m(model, out_len, start, chunk)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdosages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/.shortcut-targets-by-id/1CR_T3ueVVIodQNUaikuhRadZOrRFYizR/project/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Passing in the input and hidden state into the model and obtaining outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# out, hidden = self.rnn(x, hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 692\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seN8Jm9__iK6"
      },
      "source": [
        "#lstm _ 5  0.6187999999999999\n",
        "0.621225, 0.6254333333333334, 0.6212000000000001, 0.6289000000000001 \n",
        "0.9548500000000001, 0.9601999999999999, 0.9643999999999998, 0.9701999999999993"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmZd1bBEj4Op"
      },
      "source": [
        "\n",
        "def get_features(features, gene):\n",
        "    return (features + gene) % 3\n",
        "\n",
        "ref_panel = torch.load(\"data/ref_panel_10000.t\")\n",
        "Nref = 200\n",
        "num_chunks = 5\n",
        "M = 500\n",
        "\n",
        "def sample(model, out_len, start, chunk):\n",
        "\n",
        "    m = len(start)\n",
        "    train_seq = torch.zeros((1, m, Nref))\n",
        "\n",
        "    for i in range(train_seq.shape[0]):\n",
        "        for j in range(m):\n",
        "            train_seq[i, j, :] = get_features(ref_panel[:, M * chunk + j], start[j]) - 1\n",
        "\n",
        "    model.eval()\n",
        "    ind = len(start)\n",
        "\n",
        "    for _ in range(out_len - len(start)):\n",
        "        train_seq = train_seq.to(\"cuda\")\n",
        "        out, hidden = model(train_seq)\n",
        "        prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "        gene = torch.max(prob, dim=0)[1].item()\n",
        "\n",
        "        start.append(gene)\n",
        "        feat = get_features(ref_panel[:, M * chunk + ind], gene)\n",
        "        feat = feat.reshape(1, 1, len(feat)).to(\"cuda\") - 1\n",
        "        ind += 1\n",
        "        train_seq = torch.cat((train_seq, feat), dim=1)\n",
        "\n",
        "    return torch.tensor(start)\n",
        "\n",
        "model = torch.load(\"model_best.pth\")\n",
        "test_data = torch.load(\"data/test_data_10000.t\")\n",
        "\n",
        "def accuracy(lst1, lst2):\n",
        "    n = len(lst1)\n",
        "    return np.sum(lst1 == lst2)/n\n",
        "\n",
        "\n",
        "# for data in test_data:\n",
        "#     # print(data.shape)\n",
        "#     dat = data.tolist()\n",
        "#     inp_data = dat[0:100]\n",
        "#     generated = sample(model, 500, inp_data)\n",
        "#     acc = accuracy(np.array(generated[100:]), np.array(dat[100:]))\n",
        "#     # print(generated, dat)\n",
        "#     print(c, \"Accuracy: \", acc)\n",
        "#     c += 1\n",
        "#     total_acc += acc\n",
        "#     # break\n",
        "#     # exit(0)\n",
        "# print(\"Average Acc: \", total_acc/len(test_data))\n",
        "\n",
        "total_acc = 0\n",
        "c = 0\n",
        "for data in test_data:\n",
        "    # print(data.shape)\n",
        "    dat = data.tolist()\n",
        "    local_acc = 0.0\n",
        "    for chunk in tqdm(range(num_chunks)):\n",
        "      inp_data = dat[M * chunk : M * chunk + 100]\n",
        "      generated = sample(model, 500, inp_data, chunk)\n",
        "      acc = accuracy(np.array(generated[100:]), np.array(dat[M * chunk + 100: M * (chunk + 1)]))\n",
        "      # print(generated, dat)\n",
        "      local_acc += acc\n",
        "    local_acc /= num_chunks\n",
        "    print(c, \"Accuracy: \", local_acc)\n",
        "    c += 1\n",
        "    total_acc += local_acc\n",
        "\n",
        "print(\"Average Acc: \", total_acc/len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-crmqc_5ZYmk"
      },
      "source": [
        "!python3 test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dypsgd7-cSSY"
      },
      "source": [
        "!python3 data.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}